## Pandas Tutorial

"""
Using Pokemon Dataset:Example - 1
"""
# Part - 1 (Basics)
## Loading different types of data into Pandas and observe the change

import pandas as pd

# csv format
df = pd.read_csv('pokemon_data.csv')   
print(df.tail(3))

# xlsx format
#df_xlsx = pd.read_excel('pokemon_data.xlsx')
#print(df.tail(3))


# txt format
#df_txt = pd.read_csv('pokemon_data.txt', delimiter = '\t')
#print(df.head(3))

# ===================================================================================================
## Read headers

print(df.columns)
print('-'* 80)

## Read each column

print(df['Name'].head(3))  # (or) print(df.Name)
print('-'* 80)

print(df[['Name','Type 1', 'HP']].head(3))
print('-'* 80)

## Read each row

print(df.iloc[:4, :5])
print('-'* 80)

for index, row in df.iterrows():
    print(index, row['Name'])

print('-'* 80)

print(df.loc[df['Type 1'] == 'Fire'])
print('-' * 80)

## Read a specific location (R,C)

print(df.iloc[3,1])

# ===================================================================================================
## Desrcibe/sorting data

df.describe()   # find the mean, std - numeric data

df.sort_values(df['Name']) # (or) df.sort_values(df['Name'], ascending = True)
df.sort_values(df['Type 1','HP'], ascending = [1, 0])


## Making Changes into the data
df['Total'] = df['HP'] + df['Attack'] + ... + df['Speed']

df = df.drop(column = ['Total'])

df['total'] = df.iloc[:,4:10].sum()

# one way to rearranging the columns as per wish
cols = list(df.columns)
df = df[cols[0:4] + [cols[-1]] + cols[4:11]]

# ===================================================================================================
## saving the modofied data in three different format
df.to_csv('modified.csv') # (or) df.to_csv('modified.csv', index = False)

df.to_excel('modified.xlsx', index = False)

df.to_csv('modified.txt', index = False, sep = '\t')

# ===================================================================================================

# Part - 2 (More adavnce function)
# Filtering data

new_df = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]

new_df.to_csv('filtered')

new_df.reset_index()

new_df.reset_index(drop = True, inplace = True)

# contain delection - eg mega

df.loc[df.Name.str.contains('Mega')]

# now to delete the name contain mega use ~

df.loc[~ df.Name.str.contains('Mega')]

# Some complicated filtering - using re (regular experssion)

import re

df.loc[df['Type 1'].str.contains('fire|grass', regex = True)]  # Observe the change

df.loc[df['Type 1'].str.contains('Fire|Grass', regex = True)]

# ignoring the capatiliztaion

df.loc[df['Type 1'].str.contains('fire|grass', flags = re.I, regex = True)]

df.loc[df['Name'].str.contains('pi[a-z]*', flags = re.I, regex = True)]

df.loc[df['Name'].str.contains('pi[a-z]*', flags = re.I, regex = True)]

# ===================================================================================================

# Conditioning Changes

df.loc[df['Type 1'] == 'Fire', 'Type 1'] = 'Flamer'

df.loc[df['Type 1'] == 'Flamer', 'Type 1'] == 'Fire' # change back to fire

df.loc[df['Type'] > 500 , [Generation, Legendary]] = 'Test Value'

df.loc[df['Type 1'] == 'Fire' , ['Generation', 'Legendary']] = 'Test Value'

df.loc[df['Type 1'] == 'Fire' , ['Generation', 'Legendary']] = ['Test 1', 'Test 2']

# load the modified check data

df.loc[df['Total'] > 500 , ['Generation, 'legendary']] = 'Test value'

# ===================================================================================================

# Aggregate Statistics (Groupby)  - use .groupby function

df.groupby(['Type 1']).mean()

df.groupby(['Type 1']).mean().sort_values('Defense', ascending = False)

df.groupby(['Type 1']).sum()  # one must knows the reason they they use to use this function

df.groupby(['Type 1']).count()

# add a column on the df 
df['count'] = 1  # 1 is default value as we cant create a empty col

df.groupby(['Type 1']).count()

df.groupby(['Type 1']).count()['count']

df.groupby(['Type 1', 'Type 2']).count()['count']  # group by multiple parameter

# ===================================================================================================

# Working with large amounts of data

59:00mins

